# Обучение LLM-модели: видеомонтаж

**Куликов Дмитирй Сергеевич**  
**Группа: 21612**  
**Тема: "Обучение LLM с помощью LoRA на специализированном датасете"**

Ссылка на colab: https://colab.research.google.com/drive/1g6rVFZRSPiyTh-LkjOeGaJOx8ZO7vMHf?usp=sharing

---

## Цель проекта

Дообучение **Qwen2.5-0.5B-Instruct** с помощью **LoRA** на датасете из **150 примеров** по видеомонтажу.  
Демонстрация перехода от **общей модели** → **начинающий по Premiere Pro/DaVinci Resolve**.

## 1. Выбор модели: Qwen2.5-0.5B-Instruct 

| Характеристика | Значение |
|----------------|----------|
| **Параметры** | 500M (0.5B) |
| **Размер** | ~1GB |
| **Контекст** | 32K токенов |
| **Русский язык** | Есть |


**Почему выбрал:**
- **Компактная** 
- **Instruct-версия** → специализированная модель для обучения на выборке "Вопрос-ответ"
- **Qwen2.5** → хороший выбор с поддержкой русского языка
- Нужна была **"простенькая"** модель, которая значительно слабее других.

Модель загружается следующим образом:
`base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)
`

## 2. Датасет: Видеомонтаж (150 примеров)

**Тема:** Терминология и рабочие процессы видеомонтажа

Модель обучается узкоспециализированной области — видеомонтаж, включая:
терминологию,
-профессиональные инструменты,
-приёмы монтажа,
-цветокоррекцию,
-саунд-дизайн,
-Adobe Premiere Pro / DaVinci Resolve / After Effects.

**Тестовые промпты (`test_questions.txt`):**
- "Видеомонтаж — это"
- "Основная задача видеомонтажа заключается в том, что"
- "Хороший видеомонтаж отличается от любительского тем, что"
- "Для видеомонтажа чаще всего используют программы"
**Используется формат JSONL, совместимый с HuggingFace: примеры из `data.jsonl`(датасета)**
- {"prompt": "Что такое видеомонтаж?", "response": "Видеомонтаж — это процесс создания единого видео из отдельных клипов путём их нарезки, склейки, добавления эффектов и звука."}
- {"prompt": "В чём основная задача видеомонтажа?", "response": "Основная задача видеомонтажа — создать логически связанный и эмоционально воздействующий рассказ из сырых видеоматериалов."}
- {"prompt": "Чем хороший видеомонтаж отличается от любительского?", "response": "Хороший видеомонтаж отличается от любительского тем, что соблюдает ритм, использует плавные переходы, точно синхронизирует звук и имеет чистый монтажный ритм."}
- {"prompt": "С чего обычно начинается процесс монтажа?", "response": "Процесс монтажа обычно начинается с импорта видеофайлов в монтажную программу и создания черновой последовательности клипов."}
- {"prompt": "Что необходимо для качественного видеомонтажа?", "response": "Для качественного видеомонтажа в первую очередь необходимо мощный компьютер с хорошей видеокартой и большим объёмом оперативной памяти."}

**Эти же промпты** применяются **до и после обучения**, что позволяет объективно сравнить результа

## 3. Обучение
В этом проекте используется **LoRA (Low-Rank Adaptation)** для параметро-эффективного дообучения языковой модели. Ниже приведены наиболее важные строки кода, определяющие корректность и качество обучения.

### **3.1. Загрузка базовой модели**
`base_model = AutoModelForCausalLM.from_pretrained(
model_name, # Qwen/Qwen2.5-0.5B-Instruct
torch_dtype=torch.float16, # ↓ VRAM с 2GB до 1GB
device_map="auto" # Авто-распределение по GPU
)`

**Назначение:**
- Загружает **instruction-модель** для генерации текста
- `float16` снижает потребление видеопамяти **в 2 раза**
- `device_map="auto"` автоматически распределяет модель на GPU

**3.2. Формат обучающих данных**
`{"prompt": "Что такое видеомонтаж?", "response": "процесс создания единого видео из отдельных клипов..."}`
**Назначение:**
- Обучает модель **отвечать на вопросы**
- Формирует связь `вопрос → корректный ответ`
- **Критично** для instruction-моделей (chat/instruct)

**3.3 Использование LoRA**
`--use_lora`
- Активирует параметро-эффективное обучение без изменения базовых весов модели.

**3.4. Основные параметры LoRA**
`--lora_r 32
--lora_alpha 64`

**Назначение:**
- `lora_r=32` — размер "узкого горлышка" адаптации
- `lora_alpha=64` — коэффициент влияния LoRA-весов

**3.5. Параметры обучения**
`--learning_rate 2e-4 
--num_train_epochs 10`

**Назначение:**
- `2e-4` — **оптимальная** скорость для LoRA (не переучивает)
- `10 эпох` — достаточно для усвоения **100 примеров** домена
